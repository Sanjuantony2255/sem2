{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a78a5e8-1665-40c7-9965-5d6f7e3037c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cc2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cc2/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/cc2/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text : \n",
      " A newspaper is the strongest medium for news. People are reading newspapers for decades. It has a huge contribution to globalization. Right now because of easy internet connection, people don't read printed newspapers often. They read the online version. \n",
      "\n",
      "Tokenizing by sentence: \n",
      " ['A newspaper is the strongest medium for news.', 'People are reading newspapers for decades.', 'It has a huge contribution to globalization.', \"Right now because of easy internet connection, people don't read printed newspapers often.\", 'They read the online version.'] \n",
      "\n",
      "Tokenizing by word: \n",
      " ['A', 'newspaper', 'is', 'the', 'strongest', 'medium', 'for', 'news', '.', 'People', 'are', 'reading', 'newspapers', 'for', 'decades', '.', 'It', 'has', 'a', 'huge', 'contribution', 'to', 'globalization', '.', 'Right', 'now', 'because', 'of', 'easy', 'internet', 'connection', ',', 'people', 'do', \"n't\", 'read', 'printed', 'newspapers', 'often', '.', 'They', 'read', 'the', 'online', 'version', '.'] \n",
      "\n",
      "After filtering the stop words and punctuation : \n",
      "newspaper\n",
      "strongest\n",
      "medium\n",
      "news\n",
      "People\n",
      "reading\n",
      "newspapers\n",
      "decades\n",
      "huge\n",
      "contribution\n",
      "globalization\n",
      "Right\n",
      "easy\n",
      "internet\n",
      "connection\n",
      "people\n",
      "n't\n",
      "read\n",
      "printed\n",
      "newspapers\n",
      "often\n",
      "read\n",
      "online\n",
      "version\n",
      "Given words:  ['reading', 'globalization', 'Being', 'Went', 'gone', 'going']\n",
      "After stemming:  ['read', 'global', 'be', 'went', 'gone', 'go'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/cc2/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks rock\n",
      "corpora : corpus\n",
      "better: better\n",
      "believes : belief \n",
      "\n",
      "better: went\n",
      "better go\n",
      "better: went \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cc2/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Error loading maxent ne_chunker: Package 'maxent\n",
      "[nltk_data]     ne_chunker' not found in index\n",
      "[nltk_data] Downloading package words to /home/cc2/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging: \n",
      "\n",
      "('A', 'DT')\n",
      "('newspaper', 'NN')\n",
      "('is', 'VBZ')\n",
      "('the', 'DT')\n",
      "('strongest', 'JJS')\n",
      "('medium', 'NN')\n",
      "('for', 'IN')\n",
      "('news', 'NN')\n",
      "('.', '.')\n",
      "('People', 'NNS')\n",
      "('are', 'VBP')\n",
      "('reading', 'VBG')\n",
      "('newspapers', 'NNS')\n",
      "('for', 'IN')\n",
      "('decades', 'NNS')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('has', 'VBZ')\n",
      "('a', 'DT')\n",
      "('huge', 'JJ')\n",
      "('contribution', 'NN')\n",
      "('to', 'TO')\n",
      "('globalization', 'NN')\n",
      "('.', '.')\n",
      "('Right', 'RB')\n",
      "('now', 'RB')\n",
      "('because', 'IN')\n",
      "('of', 'IN')\n",
      "('easy', 'JJ')\n",
      "('internet', 'JJ')\n",
      "('connection', 'NN')\n",
      "(',', ',')\n",
      "('people', 'NNS')\n",
      "('do', 'VBP')\n",
      "(\"n't\", 'RB')\n",
      "('read', 'VB')\n",
      "('printed', 'JJ')\n",
      "('newspapers', 'NNS')\n",
      "('often', 'RB')\n",
      "('.', '.')\n",
      "('They', 'PRP')\n",
      "('read', 'VBD')\n",
      "('the', 'DT')\n",
      "('online', 'JJ')\n",
      "('version', 'NN')\n",
      "('.', '.')\n",
      "\n",
      "\n",
      "After Chunking:\n",
      " (S\n",
      "  (NP A/DT newspaper/NN)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  strongest/JJS\n",
      "  (NP medium/NN)\n",
      "  for/IN\n",
      "  (NP news/NN)\n",
      "  ./.\n",
      "  People/NNS\n",
      "  are/VBP\n",
      "  reading/VBG\n",
      "  newspapers/NNS\n",
      "  for/IN\n",
      "  decades/NNS\n",
      "  ./.\n",
      "  It/PRP\n",
      "  has/VBZ\n",
      "  (NP a/DT huge/JJ contribution/NN)\n",
      "  to/TO\n",
      "  (NP globalization/NN)\n",
      "  ./.\n",
      "  Right/RB\n",
      "  now/RB\n",
      "  because/IN\n",
      "  of/IN\n",
      "  (NP easy/JJ internet/JJ connection/NN)\n",
      "  ,/,\n",
      "  people/NNS\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  read/VB\n",
      "  printed/JJ\n",
      "  newspapers/NNS\n",
      "  often/RB\n",
      "  ./.\n",
      "  They/PRP\n",
      "  read/VBD\n",
      "  (NP the/DT online/JJ version/NN)\n",
      "  ./.)\n",
      "                                                                                                                                                                                       S                                                                                                                                                                                                                                   \n",
      "   ____________________________________________________________________________________________________________________________________________________________________________________|___________________________________________________________________________________________________________________________________________________________________________________________________________________                 \n",
      "  |      |          |         |     |      |         |         |            |          |         |       |    |       |      |    |     |       |        |        |    |      |        |      |       |        |            |           |      |     |        |      |        NP                  NP       NP           NP                          NP                     NP                              NP              \n",
      "  |      |          |         |     |      |         |         |            |          |         |       |    |       |      |    |     |       |        |        |    |      |        |      |       |        |            |           |      |     |        |      |    ____|_______            |        |      ______|___________                |             _________|____________           ________|_________       \n",
      "is/VBZ the/DT strongest/JJS for/IN ./. People/NNS are/VBP reading/VBG newspapers/NNS for/IN decades/NNS ./. It/PRP has/VBZ to/TO ./. Right/RB now/RB because/IN of/IN ,/, people/NNS do/VBP n't/RB read/VB printed/JJ newspapers/NNS often/RB ./. They/PRP read/VBD ./. A/DT     newspaper/NN medium/NN news/NN a/DT huge/JJ contribution/NN globalization/NN easy/JJ internet/JJ connection/NN the/DT online/JJ version/NN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#Tokenizing\n",
    "from nltk.tokenize import *\n",
    "text=\"\"\"A newspaper is the strongest medium for news. People are reading newspapers for decades. It has a huge contribution to globalization. Right now because of easy internet connection, people don't read printed newspapers often. They read the online version.\"\"\"\n",
    "print(\"Sample text : \\n\",text,\"\\n\")\n",
    "sent_tokenized=sent_tokenize(text)\n",
    "print(\"Tokenizing by sentence: \\n\",sent_tokenized,\"\\n\")\n",
    "word_tokenized=word_tokenize(text)\n",
    "print(\"Tokenizing by word: \\n\",word_tokenized,\"\\n\")\n",
    "#Filtering stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "stopwords=stopwords.words('english')\n",
    "punctuation=list(punctuation)\n",
    "print(\"After filtering the stop words and punctuation : \")\n",
    "for word in word_tokenized:\n",
    "    if word.casefold() not in stopwords and word.casefold() not in punctuation:\n",
    "        print(word)\n",
    "#new_list=[word for word in word_tokenized if word.case fold() not in stopwords and word not in punctuation]\n",
    "#print(new_list,\"\\n\")\n",
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words = [\"reading\", \"globalization\", \"Being\", \"Went\",\"gone\",\"going\"]\n",
    "print(\"Given words: \",words)\n",
    "stemm=[ps.stem(i) for i in words ]\n",
    "print(\"After stemming: \",stemm,\"\\n\")\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lem=WordNetLemmatizer()\n",
    "print(\"rocks\", lem.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lem. lemmatize(\"corpora\"))\n",
    "print(\"better:\", lem.lemmatize(\"better\"))\n",
    "print(\"believes :\", lem.lemmatize(\"believes\"),\"\\n\")\n",
    "print(\"better:\", lem.lemmatize(\"went\",pos=\"a\"))\n",
    "print(\"better\", lem.lemmatize(\"went\",pos=\"v\"))\n",
    "print(\"better:\", lem.lemmatize(\"went\",pos=\"n\"),\"\\n\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import RegexpParser\n",
    "from nltk.tree import *\n",
    "#POS Tag\n",
    "postag=nltk.pos_tag(word_tokenized)\n",
    "print(\"POS tagging: \\n\")\n",
    "for i in postag:\n",
    "    print(i)\n",
    "#Chunking\n",
    "print(\"\\n\")\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunker = RegexpParser(grammar)\n",
    "output = chunker.parse(postag)\n",
    "print(\"After Chunking:\\n\",output)\n",
    "output.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11670b9-43f4-4a60-8e30-4fff9e536215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47320ea5-f372-47b7-9e3c-c33777663032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
